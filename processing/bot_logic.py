# -*- coding: utf-8 -*- 

import os
import shutil
from dotenv import load_dotenv
from pathlib import Path
import logging

load_dotenv()
import subprocess
import random
from moviepy.editor import (
    VideoFileClip,
    TextClip,
    CompositeVideoClip,
    ColorClip,
    clips_array,
    vfx,
    concatenate_videoclips,
)
import json
from openai import OpenAI
import time
import tempfile
import re
import cv2
import numpy as np
from faster_whisper import WhisperModel
from processing.transcription import get_transcript_segments_and_file, get_audio_duration
from processing.subtitles import create_subtitle_clips, get_subtitle_items


client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

from pytubefix import YouTube

logger = logging.getLogger(__name__)


def format_seconds_to_hhmmss(seconds):
    seconds = float(seconds)
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = seconds % 60
    return f"{h:02d}:{m:02d}:{s:04.1f}"

def to_seconds(t: str) -> float:
    h, m, s_part = t.split(':')
    s = float(s_part)
    return int(h) * 3600 + int(m) * 60 + s

def get_unique_output_dir(base="output"):
    n = 1
    while True:
        out_dir = f"{base}{n}"
        if not Path(out_dir).exists():
            Path(out_dir).mkdir(parents=True)
            return out_dir
        n += 1

def gpt_gpt_prompt(shorts_number):
    prompt = ( '''
–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –Ω–∞ —Ñ–∞–±—Ä–∏–∫–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è TikTok, YouTube Shorts –∏ Instagram Reels.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ (—à–æ—É, –∏–Ω—Ç–µ—Ä–≤—å—é, –ø–æ–¥–∫–∞—Å—Ç, —Å—Ç—Ä–∏–º) –≤—ã–±—Ä–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–∏—Ä–∞–ª—å–Ω—ã–µ, —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏ —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –Ω–∞–±—Ä–∞—Ç—å –º–∏–ª–ª–∏–æ–Ω—ã –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤.
''')
    if shorts_number != 'auto':
        prompt += f"–ù–∞–π–¥–∏ —Ä–æ–≤–Ω–æ {shorts_number} —Å–∞–º—ã—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø–æ–¥ —ç—Ç–∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏.\n\n"
    
    prompt += ('''
–ñ—ë—Å—Ç–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞:

–î–ª–∏–Ω–∞ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–ø–∞: –æ—Ç 00:10 –¥–æ 01:00.
–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 20‚Äì45 —Å–µ–∫—É–Ω–¥.
–ù–∏ –æ–¥–∏–Ω –∫–ª–∏–ø –Ω–µ –¥–æ–ª–∂–µ–Ω –æ–±—Ä—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ –º—ã—Å–ª–∏ –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
–ö–ª–∏–ø –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–Ω—è—Ç–µ–Ω –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—Å–µ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é.
–ï—Å–ª–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∏–ø –ø–æ–ª—É—á–∏–ª—Å—è <10 —Å–µ–∫—É–Ω–¥, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä—å –µ–≥–æ –∑–∞ —Å—á—ë—Ç —Å–æ—Å–µ–¥–Ω–∏—Ö —Ä–µ–ø–ª–∏–∫ (–≤–ø–µ—Ä—ë–¥ –∏–ª–∏ –Ω–∞–∑–∞–¥), —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å–º—ã—Å–ª–æ–≤—É—é —Ü–µ–ª—å–Ω–æ—Å—Ç—å.

–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–±–æ—Ä–∞:
–≠–º–æ—Ü–∏–∏ ‚Äî —Å–º–µ—Ö, —à—É—Ç–∫–∏, —Å–∞—Ä–∫–∞–∑–º, –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã, –ø—Ä–∏–∑–Ω–∞–Ω–∏—è.
–ü—Ä–æ–≤–æ–∫–∞—Ü–∏—è ‚Äî –æ—Å—Ç—Ä—ã–µ –º–Ω–µ–Ω–∏—è, —Å–ø–æ—Ä–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, —Å–∫–∞–Ω–¥–∞–ª—å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã.
–¶–∏—Ç–∞—Ç—ã –∏ –º–µ—Ç–∞—Ñ–æ—Ä—ã ‚Äî —Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –ª–µ–≥–∫–æ –≤—ã–Ω–µ—Å—Ç–∏ –Ω–∞ –ø—Ä–µ–≤—å—é.
–ò—Å—Ç–æ—Ä–∏–∏ ‚Äî –º–∏–Ω–∏-–Ω–æ–≤–µ–ª–ª—ã, –∞–Ω–µ–∫–¥–æ—Ç—ã, —Ä–∞—Å—Å–∫–∞–∑—ã.
–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å ‚Äî —Å–æ–≤–µ—Ç—ã, –ª–∞–π—Ñ—Ö–∞–∫–∏, –ø—Ä–∞–≤–∏–ª–∞ —É—Å–ø–µ—Ö–∞.
–°–∂–∞—Ç–æ—Å—Ç—å ‚Äî –∑—Ä–∏—Ç–µ–ª—å –¥–æ–ª–∂–µ–Ω –ø–æ–Ω—è—Ç—å —Å—É—Ç—å –∑–∞ –ø–µ—Ä–≤—ã–µ 3 —Å–µ–∫—É–Ω–¥—ã —Ä–æ–ª–∏–∫–∞.

–§–∞–π–ª —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω (—Ñ–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–∫: `ss.s --> ss.s` + —Ç–µ–∫—Å—Ç)
–û—Ç–≤–µ—Ç ‚Äî –°–¢–†–û–ì–û JSON-–º–∞—Å—Å–∏–≤:

[{"start":"SS.S","end":"SS.S","hook":"–∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"}]

–í hook –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π –Ω–∞—á–∞–ª–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞. –ü–∏—à–∏ –≥–æ—Ç–æ–≤—ã–π –∫–ª–∏–∫–±–µ–π—Ç-–∑–∞–≥–æ–ª–æ–≤–æ–∫.
–£–±–µ–¥–∏—Å—å, —á—Ç–æ –∫–∞–∂–¥—ã–π –∫–ª–∏–ø –¥–æ–ª—å—à–µ 10 —Å–µ–∫—É–Ω–¥.
''')
    return prompt

# --- YouTube –∑–∞–≥—Ä—É–∑–∫–∞ ---
def check_video_availability(url: str) -> (bool, str, str):
    """
    Checks if a YouTube video is available without downloading it.
    Returns a tuple (is_available, message).
    """
    try:
        yt = YouTube(url)
        # Accessing the title is a lightweight way to check for availability
        _ = yt.title
        # Check if there are any streams available
        if not yt.streams:
            return False, "–í–∏–¥–µ–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –¥–ª—è –Ω–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è.", "no streams"
        return True, "–í–∏–¥–µ–æ –¥–æ—Å—Ç—É–ø–Ω–æ.", "Video is available"
    except Exception as e:
        error_message = f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –≤–∏–¥–µ–æ: {e}"
        print(error_message)
        if "age restricted" in str(e).lower():
            return False, "‚ö†Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å ‚Äì YouTube –ø–æ–º–µ—Ç–∏–ª —ç—Ç–æ—Ç —Ä–æ–ª–∏–∫ –∫–∞–∫ 18+, –∏ –¥–æ—Å—Ç—É–ø –∫ –∏—Å—Ö–æ–¥–Ω–∏–∫—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω.\n\n–í—ã–±–µ—Ä–∏ –¥—Ä—É–≥–æ–π —Ä–æ–ª–∏–∫ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π ‚Äî –∏ –º—ã –≤—Å—ë —Å–¥–µ–ª–∞–µ–º ‚ú®", "age restricted"
        if "private" in str(e).lower():
            return False, "–≠—Ç–æ –≤–∏–¥–µ–æ –ø—Ä–∏–≤–∞—Ç–Ω–æ–µ –∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–∫–∞—á–∞–Ω–æ.", "private"
        if "unavailable" in str(e).lower():
            return False, "‚ö†Ô∏è –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –º—ã –Ω–µ —Å–º–æ–≥–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —ç—Ç–æ –≤–∏–¥–µ–æ ‚Äì –≤–ª–∞–¥–µ–ª–µ—Ü —Ä–æ–ª–∏–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–∏–ª –µ–≥–æ –ø–æ–∫–∞–∑ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º –∏ –Ω–∞—à —Å–µ—Ä–≤–µ—Ä –Ω–µ –∏–º–µ–µ—Ç –∫ –Ω–µ–º—É –¥–æ—Å—Ç—É–ø–∞.\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥—Ä—É–≥–æ–µ –≤–∏–¥–µ–æ ‚Äî –≤—Å—ë –¥–æ–ª–∂–Ω–æ —Å—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ ‚úÖ\n\n–°–ø–∞—Å–∏–±–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ Shorts Factory üôå", str(e)[:100]
        return False, f"–í–∏–¥–µ–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Å—ã–ª–∫—É –∏–ª–∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–µ –≤–∏–¥–µ–æ.", str(e)[:100]

def download_video_only(url, video_path):
    """Downloads the best available video up to 720p using pytubefix."""
    try:
        yt = YouTube(url)
        # Try to get a 720p stream, otherwise get the highest resolution video-only stream
        stream = yt.streams.filter(res="1080p", progressive=False, file_extension='mp4').first()
        if not stream:
            stream = yt.streams.filter(type="video", file_extension='mp4').order_by('resolution').desc().first()
        
        if not stream:
            raise ConnectionError("No suitable MP4 video stream found by pytubefix.")

        output_dir = Path(video_path).parent
        file_name = Path(video_path).name
        stream.download(output_path=str(output_dir), filename=file_name)
        print(f"pytubefix: Video downloaded successfully to {video_path}")
        return video_path
    except Exception as e:
        print(f"An error occurred with pytubefix while downloading video: {e}")
        return None

def download_audio_only(url, audio_path, lang='ru'):
    """
    Downloads audio using pytubefix and converts it for Whisper.
    Falls back to yt-dlp if pytubefix fails.
    """
    audio_path = Path(audio_path).with_suffix(".ogg")
    temp_path = audio_path.with_suffix(".temp.mp4") # pytubefix often saves audio as .mp4

    try:
        # 1. Download best audio track with pytubefix
        print("Attempting to download audio with pytubefix...")
        yt = YouTube(url)
        stream = yt.streams.get_audio_only()
        if not stream:
            raise ConnectionError("No audio stream found by pytubefix.")
        
        stream.download(output_path=str(temp_path.parent), filename=temp_path.name)
        print(f"pytubefix: Audio downloaded successfully to {temp_path}")

    except Exception as e:
        print(f"pytubefix failed: {e}. Falling back to yt-dlp for audio.")
        try:
            format_selector = f"bestaudio[lang={lang}]/bestaudio"
            subprocess.run([
                "python3", "-m", "yt_dlp",
                "-f", format_selector,
                "--user-agent", "Mozilla/5.0",
                "-o", str(temp_path),
                url
            ], check=True)
        except subprocess.CalledProcessError as e_dlp:
            print(f"yt-dlp also failed for audio: {e_dlp}")
            return None # Both methods failed

    # 2. Convert to the required .ogg format for Whisper
    try:
        subprocess.run([
            "ffmpeg",
            "-i", str(temp_path),
            "-ac", "1",
            "-ar", "24000",
            "-c:a", "libopus",
            "-b:a", "32k",
            "-y",
            str(audio_path)
        ], check=True, capture_output=True, text=True) # Capture output to hide ffmpeg noise unless error
    except subprocess.CalledProcessError as e_ffmpeg:
        print(f"ffmpeg conversion failed: {e_ffmpeg.stderr}")
        return None

    # 3. Clean up the temporary file
    temp_path.unlink(missing_ok=True)

    return audio_path

def merge_video_audio(video_path, audio_path, output_path):

    video_path = str(video_path)
    audio_path = str(audio_path)
    output_path = str(output_path)

    # ffmpeg –∫–æ–º–∞–Ω–¥–∞: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∞—É–¥–∏–æ –≤ AAC –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å MP4
    cmd = [
        "ffmpeg",
        "-i", video_path,
        "-i", audio_path,
        "-c:v", "copy",        # –∫–æ–ø–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        "-c:a", "aac",         # –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∞—É–¥–∏–æ –≤ AAC
        "-b:a", "128k",        # –±–∏—Ç—Ä–µ–π—Ç –∞—É–¥–∏–æ
        "-shortest",           # —á—Ç–æ–±—ã –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞ –±—ã–ª–∞ —Ä–∞–≤–Ω–∞ –º–µ–Ω—å—à–µ–π –∏–∑ –≤–∏–¥–µ–æ/–∞—É–¥–∏–æ
        "-y",                  # –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –µ—Å–ª–∏ –µ—Å—Ç—å
        output_path
    ]

    subprocess.run(cmd, check=True)
    
    # —É–¥–∞–ª—è–µ–º –≤–∏–¥–µ–æ –±–µ–∑ –∑–≤—É–∫–∞
    if os.path.exists(video_path):
        os.remove(video_path)
        
    return output_path

def get_highlights_from_gpt(captions_path: str = "captions.txt", audio_duration: float = 600.0, shorts_number: any = 'auto'):
    """
    –î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Responses API (–º–æ–¥–µ–ª—å gpt-5) —Å –≤–∫–ª—é—á—ë–Ω–Ω—ã–º File Search.
    –®–∞–≥–∏: —Å–æ–∑–¥–∞—ë—Ç Vector Store, –∑–∞–≥—Ä—É–∂–∞–µ—Ç .txt, –ø—Ä–∏–∫—Ä–µ–ø–ª—è–µ—Ç –µ–≥–æ –∫ Vector Store,
    –∑–∞—Ç–µ–º –≤—ã–∑—ã–≤–∞–µ—Ç –º–æ–¥–µ–ª—å. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç [{"start":"HH:MM:SS","end":"HH:MM:SS","hook":"..."}].
    """
    prompt = gpt_gpt_prompt(shorts_number)

    # 1) —Å–æ–∑–¥–∞—ë–º Vector Store
    vs = client.vector_stores.create(name="shorts_captions_store")

    # 2) –∑–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª –∏ –ø—Ä–∏–∫—Ä–µ–ø–ª—è–µ–º –∫ Vector Store
    with open(captions_path, "rb") as f:
        uploaded = client.files.create(file=f, purpose="assistants")

    client.vector_stores.files.create(
        vector_store_id=vs.id,
        file_id=uploaded.id,
    )

    # (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ) –ø–æ–¥–æ–∂–¥—ë–º, –ø–æ–∫–∞ —Ñ–∞–π–ª –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç—Å—è
    # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö
    _wait_vector_store_ready(vs.id)

    # 3) –≤—ã–∑—ã–≤–∞–µ–º Responses API —Å –ø–æ–¥–∫–ª—é—á—ë–Ω–Ω—ã–º file_search –∏ –Ω–∞—à–∏–º vector_store
    resp = client.responses.create(
        model="gpt-5",
        input=[{"role": "user", "content": prompt}],
        tools=[{
            "type": "file_search",
            "vector_store_ids": [vs.id],
        }],
    )

    raw = _response_text(resp)
    json_str = _extract_json_array(raw)
    data = json.loads(json_str)

    # –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ: SS.S -> HH:MM:SS.S, +0.5 —Å–µ–∫ –∫ end
    items = [{
        "start": format_seconds_to_hhmmss(float(it["start"])),
        "end":   format_seconds_to_hhmmss(float(it["end"])),
        "hook":  it["hook"]
    } for it in data]

    return items

def _wait_vector_store_ready(vector_store_id: str, timeout_s: int = 30, poll_s: float = 1.0):
    """
    –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞: –∂–¥—ë–º, –ø–æ–∫–∞ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –ø–æ—è–≤—è—Ç—Å—è –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.
    –ï—Å–ª–∏ –≤–∞—à SDK –¥–∞—ë—Ç –¥–æ—Å—Ç—É–ø –∫ file_counts ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ; –∏–Ω–∞—á–µ –ø—Ä–æ—Å—Ç–æ —Å–ø–∏–º –Ω–µ–º–Ω–æ–≥–æ.
    """
    t0 = time.time()
    while time.time() - t0 < timeout_s:
        try:
            vs = client.vector_stores.retrieve(vector_store_id=vector_store_id)
            # –≤ –Ω–æ–≤—ã—Ö SDK —á–∞—Å—Ç–æ –µ—Å—Ç—å vs.file_counts.completed
            fc = getattr(vs, "file_counts", None)
            completed = getattr(fc, "completed", None) if fc else None
            if isinstance(completed, int) and completed > 0:
                return
        except Exception:
            pass
        time.sleep(poll_s)

# ===== –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =====

def _response_text(resp) -> str:
    """
    –ê–∫–∫—É—Ä–∞—Ç–Ω–æ –¥–æ—Å—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ Responses API –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö/–≤–µ—Ä—Å–∏—è—Ö SDK.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: output_text -> output[..].content[..].text -> fallback –≤ str(resp).
    """
    # 1) –ù–æ–≤—ã–π SDK –∑–∞—á–∞—Å—Ç—É—é –∏–º–µ–µ—Ç —É–¥–æ–±–Ω–æ–µ —Å–≤–æ–π—Å—Ç–≤–æ:
    if hasattr(resp, "output_text") and resp.output_text:
        return resp.output_text.strip()

    # 2) –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä content-–±–ª–æ–∫–æ–≤
    try:
        output = getattr(resp, "output", None)
        if isinstance(output, list) and output:
            # –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π item
            item = output[0]
            content = getattr(item, "content", None) or item.get("content")
            if isinstance(content, list):
                buf = []
                for c in content:
                    # –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö —Ç–µ–∫—Å—Ç –ª–µ–∂–∏—Ç –≤ c.get("text")
                    t = c.get("text") if isinstance(c, dict) else getattr(c, "text", None)
                    if isinstance(t, dict) and "value" in t:
                        buf.append(t["value"])
                    elif isinstance(t, str):
                        buf.append(t)
                if buf:
                    return "\n".join(buf).strip()
    except Exception:
        pass

    # 3) –§–æ–ª–±—ç–∫
    return str(resp)


def _extract_json_array(text: str) -> str:
    start = text.find('[')
    if start == -1:
        raise ValueError("–í –æ—Ç–≤–µ—Ç–µ GPT –Ω–µ –Ω–∞–π–¥–µ–Ω JSON-–º–∞—Å—Å–∏–≤.")
    depth = 0; in_str = False; esc = False
    for i, ch in enumerate(text[start:], start=start):
        if in_str:
            if esc: esc = False
            elif ch == '\\': esc = True
            elif ch == '"': in_str = False
        else:
            if ch == '"': in_str = True
            elif ch == '[': depth += 1
            elif ch == ']':
                depth -= 1
                if depth == 0:
                    return text[start:i+1]
    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON-–º–∞—Å—Å–∏–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ GPT.")

def get_box_center(box):
    x, y, w, h = box
    return (x + w/2, y + h/2)

def distance(p1, p2):
    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5

def create_face_tracked_clip(main_clip_raw, target_height, target_width):
    """
    Creates a clip with face tracking. The frame only moves if the speaker's face
    is about to leave the visible cropped area.
    """
    main_clip_resized = main_clip_raw.resize(height=target_height)
    
    if main_clip_resized.w <= target_width:
        return main_clip_resized

    try:
        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    except Exception as e:
        print(f"Could not load face cascade model: {e}. Falling back to center crop.")
        return main_clip_resized.fx(vfx.crop, x_center=main_clip_resized.w / 2, width=target_width)

    subclips = []
    
    crop_x_center = main_clip_resized.w / 2
    target_crop_x_center = main_clip_resized.w / 2
    crop_half_width = target_width / 2
    tracked_face_box = None

    # Smoothing factor for camera movement. Lower value = smoother/slower.
    smoothing_factor = 1.0

    step = 0.25  # Process video in chunks of 0.25 seconds
    for t in np.arange(0, main_clip_resized.duration, step):
        frame = main_clip_resized.get_frame(t)
        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        
        faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(80, 80))
        is_new_face = False

        if len(faces) > 0:
            if tracked_face_box is None:
                # Case 1: No face was tracked before. This is a new face.
                tracked_face_box = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)[0]
                is_new_face = True
            else:
                # Case 2: A face was being tracked. Find it in the new frame.
                previous_center = get_box_center(tracked_face_box)
                closest_face = min(faces, key=lambda f: distance(get_box_center(f), previous_center))
                
                max_allowed_distance = tracked_face_box[2] * 1.5
                if distance(get_box_center(closest_face), previous_center) < max_allowed_distance:
                    # The same face is still being tracked.
                    tracked_face_box = closest_face
                else:
                    # Case 3: The old face was lost. This is a new face.
                    tracked_face_box = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)[0]
                    is_new_face = True

            face_center_x, _ = get_box_center(tracked_face_box)
            
            if is_new_face:
                # INSTANT JUMP: A new face appeared, so snap the camera immediately.
                crop_x_center = face_center_x
                target_crop_x_center = face_center_x
            else:
                # SMOOTH PAN: The same face is moving. Only adjust if it nears the edge.
                face_width = tracked_face_box[2]
                visible_left = crop_x_center - crop_half_width
                visible_right = crop_x_center + crop_half_width
                buffer = face_width * 0.5

                if not (visible_left + buffer < face_center_x < visible_right - buffer):
                    target_crop_x_center = face_center_x
        else:
            # No faces detected, lose track.
            tracked_face_box = None

        # Apply smoothing towards the target. 
        # If no move is needed, target equals current, so crop_x_center stays put.
        # If it's a new face, this will have no effect since crop_x_center was already snapped.
        crop_x_center = (smoothing_factor * target_crop_x_center) + ((1 - smoothing_factor) * crop_x_center)

        # Clamp the crop_x_center to avoid black bars
        min_x = crop_half_width
        max_x = main_clip_resized.w - crop_half_width
        clamped_crop_x_center = max(min_x, min(crop_x_center, max_x))
        crop_x_center = clamped_crop_x_center
        target_crop_x_center = max(min_x, min(target_crop_x_center, max_x))

        subclip_end = min(t + step, main_clip_resized.duration)
        subclip = main_clip_resized.subclip(t, subclip_end)
        cropped_subclip = subclip.fx(vfx.crop, x_center=clamped_crop_x_center, width=target_width)
        subclips.append(cropped_subclip.without_audio())

    if not subclips:
        return main_clip_resized.fx(vfx.crop, x_center=main_clip_resized.w / 2, width=target_width)

    final_video = concatenate_videoclips(subclips)
    final_video.audio = main_clip_raw.audio
    return final_video

def _build_video_canvas(layout, main_clip_raw, bottom_video_path, final_width, final_height):
    if layout == 'square_top_brainrot_bottom':
        video_height = int(final_height * 0.6)
        bottom_height = final_height - video_height

        main_clip = create_face_tracked_clip(main_clip_raw, video_height, final_width)

        if bottom_video_path:
            full_bottom_clip = VideoFileClip(str(bottom_video_path))
            if full_bottom_clip.duration > main_clip.duration:
                random_start = random.uniform(0, full_bottom_clip.duration - main_clip.duration)
                bottom_clip = full_bottom_clip.subclip(random_start, random_start + main_clip.duration)
            else:
                bottom_clip = full_bottom_clip
            bottom_clip = bottom_clip.resize(height=bottom_height)
            if bottom_clip.w > final_width:
                bottom_clip = bottom_clip.fx(vfx.crop, x_center=bottom_clip.w / 2, width=final_width)
            bottom_clip = bottom_clip.set_duration(main_clip.duration)
        else:
            bottom_clip = ColorClip(size=(final_width, bottom_height), color=(0,0,0), duration=main_clip.duration)

        video_canvas = clips_array([[main_clip], [bottom_clip]])
        subtitle_y_pos = video_height - 60 # –°–¥–≤–∏–≥–∞–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã –≤–≤–µ—Ä—Ö
        subtitle_width = final_width - 40

    elif layout == 'full_top_brainrot_bottom':
        # Layout heights: 50/50 split
        video_container_height = final_height / 2
        bottom_height = final_height / 2

        # Main clip preparation
        main_clip = main_clip_raw.resize(width=final_width)

        # Position main_clip at the bottom of the top half
        main_clip_y_pos = video_container_height - main_clip.h
        main_clip = main_clip.set_position(('center', main_clip_y_pos))

        # Bottom clip preparation
        if bottom_video_path:
            full_bottom_clip = VideoFileClip(str(bottom_video_path))
            if full_bottom_clip.duration > main_clip.duration:
                random_start = random.uniform(0, full_bottom_clip.duration - main_clip.duration)
                bottom_clip = full_bottom_clip.subclip(random_start, random_start + main_clip.duration)
            else:
                bottom_clip = full_bottom_clip

            bottom_clip = bottom_clip.resize(height=bottom_height)
            if bottom_clip.w > final_width:
                bottom_clip = bottom_clip.fx(vfx.crop, x_center=bottom_clip.w / 2, width=final_width)
            bottom_clip = bottom_clip.set_duration(main_clip.duration)
        else:
            bottom_clip = ColorClip(size=(final_width, bottom_height), color=(0,0,0), duration=main_clip.duration)

        bottom_clip = bottom_clip.set_position(('center', 'bottom'))

        # Create final canvas
        bg = ColorClip(size=(final_width, final_height), color=(0,0,0), duration=main_clip.duration)
        video_canvas = CompositeVideoClip([bg, main_clip, bottom_clip])

        # Subtitles position
        subtitle_y_pos = video_container_height - 60
        subtitle_width = final_width - 40

    elif layout == 'full_center':
        main_clip = main_clip_raw.resize(width=final_width)
        bg = ColorClip(size=(final_width, final_height), color=(0,0,0), duration=main_clip.duration)
        video_canvas = CompositeVideoClip([bg, main_clip.set_position('center', 'center')])
        subtitle_y_pos = (final_height + main_clip.h) / 2 + 20
        subtitle_width = main_clip.w - 40

    elif layout == 'face_track_9_16':
        main_clip = create_face_tracked_clip(main_clip_raw, final_height, final_width)
        video_canvas = main_clip
        subtitle_y_pos = final_height * 0.75
        subtitle_width = final_width - 40

    else: # square_center
        video_height = int(final_height * 0.7)
        
        main_clip = create_face_tracked_clip(main_clip_raw, video_height, final_width)
        
        bg = ColorClip(size=(final_width, final_height), color=(0,0,0), duration=main_clip.duration)
        video_canvas = CompositeVideoClip([bg, main_clip.set_position('center', 'center')])
        subtitle_y_pos = final_height * 0.75
        subtitle_width = main_clip.w - 40
    
    return video_canvas, subtitle_y_pos, subtitle_width

def process_video_clips(config, video_path, audio_path, shorts_timecodes, transcript_segments, out_dir, send_video_callback=None):
    final_width = 720
    final_height = 1280
    futures = []

    # --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ ---
    subtitle_style = config.get('subtitle_style', 'white')
    layout = config.get('layout', 'square_top_brainrot_bottom')
    bottom_video_path = config.get('bottom_video_path')
    subtitles_type = config.get('subtitles_type', 'word-by-word')

    faster_whisper_model = None

    # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è faster-whisper (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ) ---
    if subtitles_type == 'word-by-word':
        faster_whisper_model = WhisperModel("base", device="cpu", compute_type="int8")

    # No longer collecting results in a list to return
    for i, short in enumerate(shorts_timecodes, 1):
        output_sub = Path(out_dir) / f"short{i}.mp4"
        start_cut = to_seconds(short["start"])
        end_cut = to_seconds(short["end"])

        current_transcript_segments = json.loads(json.dumps(transcript_segments))

        if not config.get('capitalize_sentences', True):
            for seg in current_transcript_segments:
                if seg['start'] >= start_cut:
                    text = seg['text']
                    lstripped_text = text.lstrip()
                    if lstripped_text:
                        new_text = lstripped_text[0].lower() + lstripped_text[1:]
                        seg['text'] = new_text
                if seg['start'] > end_cut:
                    break

        main_clip_raw = VideoFileClip(str(video_path)).subclip(start_cut, end_cut)

        video_canvas, subtitle_y_pos, subtitle_width = _build_video_canvas(
            layout, main_clip_raw, bottom_video_path, final_width, final_height
        )

        if subtitles_type != 'no_subtitles':
            # --- –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞–ª–æ–∂–µ–Ω–∏–µ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ ---
            subtitle_items = get_subtitle_items(
                subtitles_type, current_transcript_segments, audio_path, start_cut, end_cut, 
                faster_whisper_model)
            subtitle_clips = create_subtitle_clips(subtitle_items, subtitle_y_pos, subtitle_width, subtitle_style)
            final_clip = CompositeVideoClip([video_canvas] + subtitle_clips)
        else:
            final_clip = video_canvas
        final_clip = final_clip.set_duration(video_canvas.duration)
        final_clip.write_videofile(str(output_sub), fps=24, codec="libx264", audio_codec="aac")
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª {output_sub}")
        
        # Call the callback to send the video
        if send_video_callback:
            future = send_video_callback(file_path=output_sub, hook=short["hook"], start=short["start"], end=short["end"])
            if future:
                futures.append(future)
    return futures

def main(url, config, status_callback=None, send_video_callback=None, deleteOutputAfterSending=False, user_balance: int = None):

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ ---
    video_map = {
        'gta': './keepers/gta.mp4',
        'minecraft': './keepers/minecraft_parkur.mp4'
    }
    config['bottom_video_path'] = video_map.get(config['bottom_video'])

    out_dir = get_unique_output_dir() 
    # out_dir = './output1'
    
    print("–°–∫–∞—á–∏–≤–∞–µ–º –≤–∏–¥–µ–æ —Å YouTube...")
    # —Å–∫–∞—á–∏–≤–∞–µ–º –≤–∏–¥–µ–æ
    video_only = download_video_only(url, Path(out_dir) / "video_only.mp4")
    
    # —Å–∫–∞—á–∏–≤–∞–µ–º –∞—É–¥–∏–æ
    audio_lang = config.get('audio_lang', 'ru')
    audio_only = download_audio_only(url, Path(out_dir) / "audio_only.ogg", lang=audio_lang)
    # audio_only = Path(out_dir) / "audio_only.ogg"

    if not video_only or not audio_only:
        raise Exception("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –≤–∏–¥–µ–æ ‚Äì –º—ã —É–∂–µ –æ –Ω–µ–π –∑–Ω–∞–µ–º –∏ —Å–æ–≤—Å–µ–º —Å–∫–æ—Ä–æ –≤—Å—ë –ø–æ—á–∏–Ω–∏–º!")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ
    video_full = merge_video_audio(video_only, audio_only, Path(out_dir) / "video.mp4")
    # video_full = Path(out_dir) / "video.mp4"

    if status_callback:
        status_callback("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∏–¥–µ–æ...")
    print("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –≤–∏–¥–µ–æ...")
    force_ai_transcription = config.get('force_ai_transcription', False)
    # transcript_segments = []
    transcript_segments, lang_code = get_transcript_segments_and_file(url, out_dir=Path(out_dir), audio_path=(Path(out_dir) / "audio_only.ogg"), force_whisper=force_ai_transcription)

    if not transcript_segments:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é.")
        return 0, 0
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–º—ã—Å–ª–æ–≤—ã—Ö –∫—É—Å–∫–æ–≤ —á–µ—Ä–µ–∑ GPT
    print("–ò—â–µ–º —Å–º—ã—Å–ª–æ–≤—ã–µ –∫—É—Å–∫–∏ —á–µ—Ä–µ–∑ GPT...")
    shorts_number = config.get('shorts_number', 'auto')
    # shorts_timecodes = [
    #    { "start": '00:01:49.0', "end": "00:02:10.0", "hook": "–î–µ–Ω—å–≥–∏ –¥–æ–ª–∂–Ω—ã —Å—Ç–∞—Ç—å –±–æ–∂–µ—Å—Ç–≤–æ–º" }
    # ]
    shorts_timecodes = get_highlights_from_gpt(Path(out_dir) / "captions.txt", get_audio_duration(audio_only), shorts_number=shorts_number)
    
    if not shorts_timecodes:
        print("GPT –Ω–µ —Å–º–æ–≥ –≤—ã–¥–µ–ª–∏—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –æ—Ç—Ä–µ–∑–∫–∏ –¥–ª—è —à–æ—Ä—Ç—Å–æ–≤.")
        if status_callback:
            status_callback("GPT –Ω–µ —Å–º–æ–≥ –≤—ã–¥–µ–ª–∏—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –æ—Ç—Ä–µ–∑–∫–∏ –¥–ª—è —à–æ—Ä—Ç—Å–æ–≤.")
        return 0, 0

    if user_balance is None:
        user_balance = len(shorts_timecodes)

    num_to_process = min(len(shorts_timecodes), user_balance)
    shorts_to_process = shorts_timecodes[:num_to_process]
    extra_found = len(shorts_timecodes) - num_to_process

    if status_callback:
        status_callback(f"üî• –ù–∞–π–¥–µ–Ω—ã –æ—Ç—Ä–µ–∑–∫–∏ –¥–ª—è —à–æ—Ä—Ç—Å–æ–≤ - {len(shorts_timecodes)} —à—Ç. –°–æ–∑–¥–∞–µ–º {num_to_process} –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–æ–ª–∏–∫–æ–≤...")
    print(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ –æ—Ç—Ä–µ–∑–∫–∏ –¥–ª—è —à–æ—Ä—Ç—Å–æ–≤ ({len(shorts_timecodes)}):", shorts_timecodes)

    futures = process_video_clips(config, video_full, audio_only, shorts_to_process, transcript_segments, out_dir, send_video_callback)
    
    successful_sends = 0
    if futures:
        for future in futures:
            try:
                success = future.result() # this will block
                if success:
                    successful_sends += 1
            except Exception as e:
                print(f"A future failed when sending video: {e}")

    # –µ—Å–ª–∏ –≤—Å—ë –æ–∫, –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ—Ñ–∞–π–ª
    if os.path.exists(audio_only):
        try: os.remove(audio_only)
        except OSError: pass
    
    if deleteOutputAfterSending:
        shutil.rmtree(out_dir)
        print(f"üóëÔ∏è –ü–∞–ø–∫–∞ {out_dir} —É–¥–∞–ª–µ–Ω–∞.")

    return successful_sends, extra_found





if __name__ == "__main__":
    url = "https://youtu.be/4_3VXLK_K_A?si=GVZ3IySlOPK09Ohc"
    # ================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==================
    config = {
        # –û–ø—Ü–∏–∏: 'white', 'yellow'
        'subtitle_style': 'yellow',
        
        # –û–ø—Ü–∏–∏: 'gta', 'minecraft' –∏–ª–∏ None –¥–ª—è —á–µ—Ä–Ω–æ–≥–æ —Ñ–æ–Ω–∞
        'bottom_video': 'minecraft', 
        
        # –û–ø—Ü–∏–∏: 'square_top_brainrot_bottom', 'square_center', 'full_top_brainrot_bottom', 'full_center', 'face_track_9_16'
        'layout': 'square_center',

        # –û–ø—Ü–∏–∏: 'word-by-word', 'phrases', None
        'subtitles_type': None,

        # –û–ø—Ü–∏–∏: True, False
        'capitalize_sentences': True
    }
    # ================================================
    shorts = main(url, config)